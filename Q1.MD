1.With the aid of examples explain the rules of Big O notation in algorithm analysis:

 big o notations

 Usually expressed as n, big O notation is a common approach to explain the performance of an algorithm, particularly its execution time or memory use with respect to the size of the input.  It lets us compare several methods objectively and clarifies how an algorithm performs as the input size increases.

 
  Fundamental Guidelines and Illustrations:

 We determine the worst case:
 Big O emphasizes the slowest the algorithm can be—the worst-case situation.  In a linear search, for instance, the worst scenario is in which the intended element is either absent at all or at the very end.  This produces an O(n) complexity.

 We ignore constants; our focus is on how the algorithm scales with n rather than on the precise running count.  Thus, we reduce an algorithm running in 2n + 5 steps to O(n), whereby the 2 and the +5 are not relevant for big n.

 We just retain the dominating term: if an algorithm uses n² + n steps, we only retain the term with quickest growth as n rises.  We hence express the complexity as O(n²).

 While nested loops—a loop inside another—can produce O(n²) or higher, a single loop typically has a complexity of O(n).

 Unless they feature loops, conditionals do not add complexity:
 Unless the code within simple if/else statements has its own complexity, they have no effect on the general complexity.

 
  Java Copy Edit for (int i = 0; i < n; i++) { // basic directions}  There runs n times → the loop.  O(n) complexity


 Big O notation is, all things considered, a necessary instrument for understanding algorithms.  It enables developers—especially with a lot of data—to make better decisions when addressing challenges.  Knowing and using Big O speeds and improves code writing.